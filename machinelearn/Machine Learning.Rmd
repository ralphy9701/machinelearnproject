---
title: "Machine Learning"
author: "Pete Peters"
date: "January 25, 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Week 1
Generic ML Process: Probability/Sampling, Training/Test, Create Prediction Function, Evaluate. At lot of focus in this process is about the prediction function, but all the steps are very important.

Components of a predictor:  
  1. Question: what are you trying to predict and with what?
  2. Input Data
  3. Features (direct or created)
  4. Algorithm
  5. Retrieve parameters
  6. Evaluate parameters  
  
See the kernlab package in R for spam data (determine if an email is spam or not)

Collecting more data is almost always better than a more sophisticated model. Algorithms matter less than you think...

Features Matter. Good features lead to data compression, retain relevant information, and are created based on expert application knowledge. Common mistakes include trying to automate feature selection, not paying attention to data-specific quirks and throwing away information unnecessarily. 

What is the best ML method? Keep in mind they should be interpretable. You usually need to explain your work. Additionally, simpler is better. Accuracy obviously counts, but so does speed. Lastly, they should be scalable (applicable to a large data set either because its fast or deployable).

In and out of sample errors. 
  In sample error: the error rate you get on the same data set you used to build your predictor. This is also called resubstitution error.
  Out of sample error: the error rate you get on a new data set. Sometimes called generalization error. 
  Out of sample error is what you care about, and it's always higher. An overfit model will have much higher out of sample error to in sample error. 

Overfitting: Data has two parts, the signal and the noise. The goal is to find the signal. You can always find a perfect in-sample predictor. You capture both the signal and the noise when you do that, so your model won't perform as well on new samples. 

Prediction Study Design.  
  1. Define your error rate
  2. Split data into training, testing and validation (optional) sets
  3. On the training set, pick features and use cross-validation.
  4. On the training set, choose a prediction function and use CV
  5. If no validation, apply 1x to test set
  6. If validation, apply to test set and refine, apply 1x to validation

Rules of thumb for prediction study designs:
  If you have enough data, put 60% in training, 20% in testing and 20% in validation. If you have a small sample size, perform cross validation and report with the caveat of a small sample size. 
  The test/validation data should be set aside and NOT LOOKED AT.
  Randomly sample the training and test set.
  Your data must reflect the structure of the problem (see backtesting)
  All subsets should reflect as much diversity as possible. You can also balance by features by randomly sampling will usually solve this.
  
Types of errors. 

Basic Terms.  
  Positive = identified, negative = rejected
  True/False = correctly identified grouping
  True Positive = correctly identified
  False Positive = incorrectly identified
  True Negative = correctly rejected
  False Negative = incorrectly rejected
  Sensitivity = Pr(positive test|disease) meaning you test positive and have the disease
  Specificity = Pr(negative test|no disease)
  Positive Predictive Value = Pr(disease|positive test)
  Negative Predictive Value = Pr(no disease|negative test)
  Accuracy = Pr(correct outcome)
  
Receiver Operating Characteristic. Even when you do classification, you often generate probabilities for each group. The cutoff you choose gives different results. ROC curves are plotted with 1-specificity (P(FP)) on the x-axis and on the x-axis the sensitivity or P(TP). You are trying to make a curve where every point corresponds to a cutoff. You use the area under the curve to determine if its good. If your AUC is 1 you have a perfect classifier. If its .5, then you are guessing. They are used to pick out predicters for binary numbers.

Cross Validation. One of the most widely used tools to evaluate features and verify parameters. Accuracy is often optimistic on the training set. A better estimate comes from an independent set (but then the test set becomes a training set). So we estimate the test set accuracy with the training set. CV approach:  
  1. With the training set...
  2. Split into test and training sets (either random or k-fold, leave one out)
  3. Build a model with the training set
  4. Evaluate on the test set. 
  5. Repeat and average the estimated errors.
  
CV is used for picking variables, picking the type of prediction function to use, picking parameters in the prediction function and comparing different predictors.

For time-series data, you must pull out chunks to use as tests. For k-fold, you will get less bias and more variance with larger K, while a smaller K will give you more bias and less variance (bias is accuracy). Random sampling must be done WITHOUT REPLACEMENT. Random sampling with replacement is the bootstrap. Bootstrapping will underestimate the error. It can be corrected but its complicated. If you CV to pick predictors estimate you must estimate errors on the independent data. 

Unrelated data is the most common mistake in ML.

## Week 2.
Caret. Can be used to preProcess (clean). Data splitting and time slices can be done and training and test sets can be created. You can also do model comparison in Caret. There are a lot of machine learning algorithms in R to do just about anything you want to do. Caret standardizes most of these functions by serving as a wrapper.

```{r}
# example
# split data
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y = spam$type, p =.75, list = FALSE)
training <- spam[inTrain, ]
testing <- spam[-inTrain, ]
dim(training)

# fit a model
modelFit <- train(type ~., data = training, method = "glm")
modelFit
modelFit$finalModel

# prediction
predictions <- predict(modelFit, newdata = testing)
predictions

# evaluate the model
confusionMatrix(predictions, testing$type)

```

### Data Slicing
Data slicing is used to create the test/training sets and also with the CV/bootstrapping within the training models. See example above.

```{r}
# kfold example
folds <- createFolds(y = spam$type, k = 10, list = TRUE, returnTrain = TRUE) #list = TRUE gives you a list of indices, returnTrain = False will return the test set samples
sapply(folds, length)
folds[[1]][1:10]

# resampling
folds <- createResample(y = spam$type, times = 10, list = TRUE)

# time slices (for forecasting)
tme <- 1:1000
folds <- createTimeSlices(y = tme, initialWindow = 20, horizon = 10)
# 20 samples in each window and the test data is the next 10 points
names(folds)
```

### Training Options.
With the 75% of training data from the Spam dataset we used before, we fit a glm model. There are a lot of options with your train function. See `args(train.default)`. The metric options include RMSE and Rsquared. For categorical outcomes, you can calculate accuracy and kappa, a measure of concordance. Kappa is a more in depth measure. `traincontrol()` gives you even more control of how you train your models. `traincontrol` is also where you set resampling options. Note that this can really slow things down if you have a lot of data. Remember to set a seed, but there is a random number set when you sample from the training set so there needs to be a seed for each of these random draws. This is especially useful for parallel fits.

```{r}
set.seed(1235)
modelFit3 <- train(type~., data = training, method = "glm")
modelFit3
```

### Plotting Predictors. 
This helps you to understand how the variables interact with one another. We will use the wage data from the ISLR datset.

```{r}
library(ISLR); library(ggplot2); library(caret)
data(Wage)
summary(Wage)
# pull out the test set before you do anythign!
inTrain <- createDataPartition(y = Wage$wage, p = .7, list = FALSE)
training <- Wage[inTrain, ]
testing <- Wage[-inTrain, ]
dim(training); dim(testing)

featurePlot(x = training[, c("age", "education", "jobclass")], 
            y = training$wage, 
            plot = "pairs") # from caret
# the last plot shows all of the variables plotted against each other. you are looking for any relationships with y
qplot(age, wage, data = training)
qplot(age, wage, colour = jobclass, data = training)
# most of the people in that upper chunk are information type jobs
qq <- qplot(age, wage, color = education, data = training)
qq + geom_smooth(method = "lm", formula = y~x)
# allows you to see if there is a different relationship for different education levels
cutWage <- cut2(training$wage, g = 3) # Hmisc, # breaks into factors based on quantile groups
table(cutWage)
p1 <- qplot(cutWage, age, data = training, fill = cutWage,
            geom = c("boxplot"))
p1
table(cutWage, training$jobclass)
prop.table(table(cutWage, trianing$jobclass), 1)
qplot(wage, colour = education, data = training, geom = "density") # get a density plot of each education level

```

Don't use the test set for exploration. Look for imbalance in the outcomes and predictors. Look for outliers and groups of points not explained by a predictor. Also look out for skewed variables. 

### Preprocessing
Split the data and set the test data aside.

```{r}
hist(training$capitalAve, xlab = "ave. capital run length")
# the histogram is not useful because most are very small. Its got a very high skew so you need to preprocess.
mean(training$capitalAve)
sd(training$capitalAve) # sd is large, highly variable, lets standardize

# standardizing
trainCapAve <- training$capitalAve
trainCapAveS <- (trainCapAve - mean(trainCapAve))/sd(trainCapAve)
mean(trainCapAveS) # should be 0, sd should be 1 so you've reduced a lot of the variability in that variable.

# You'll need to apply the same transformation to the test set, but you need to use the mean and sd of the training set to do so.

# The preprocess function will do standardization for you. Below we center and scale (standardize) each variable.
preObj <- preProcess(training[, -58], method = c("center", "scale"))
trainCapAveS <- predict(preObj, training[, -58])$capitalAve
mean

# You can pass as an argument to train() as well
modelFit <- train(type ~., data = training, 
                  preProcess = c("center", "scale"), method = "glm")
modelFit

# other transformations
# Box Cox
preObj <- preProcess(training[, -58], method = c("BoxCox"))

# Imputing data
# make some values NA
training$capAve <- training$capitalAve
selectNA <- rbinom(dim(training)[1], size = 1, prob = .05) ==1
training$capAve[selectNA] <- NA
#Impute and standardize 
preObj <- preProcess(training[, -58], method = "knnImpute")
capAve <- predict(preObj, training[-58])$capAve

# Standardize true Values
capAveTruth <- training$capitalAve
capAveTruth <- (capAveTruth-mean(capAveTruth))/sd(capAveTruth)

```

### Covariate Creation
Covariates, predictors, features, or the variables you include in your model that you will use to make a prediction. They come in two levels: raw data and transformed (tidy). You want them to describe the data as much as possible while allowing it to describe the depenedent variable. (see google ngrams) Keep as much information as possible while summarizing as much as you can...balance. When in doubt, err on the side of more features. Tidy covariates are created from the initial raw data covariates. You will only perform the tidyzation on the training set (but add them to the test set when you apply your predictor). In R, each of the new covariates are added to the dataframe. 

You can google "feature extraction for "datatype"" and get some good insight. Deep learning will help automate feature creation for things like images and voices. Do some indepth EDA when creating your covariates. Be careful with overfitting/collinearity. 

### Preprocessing with Principle Components Analysis

```{r}
library(ISLR); library(caret); data(Wage)
inTrain <- createDataPartition(y = Wage$wage, p = .7, list = FALSE)
training <- Wage[inTrain, ]; testing <- Wage[-inTrain, ]

# adding dummy variables
# converts factor variables to indicator variables
table(training$jobclass) # if you plug in directly, you are adding a text datapoint, so lets make it 1s and 0s with dummyVars()
dummies <- dummyVars(wage~jobclass, data = training)
head(predict(dummies, newdata = training)) # gives you two new variables, note that you added both factors as headers, but you wont need them both

# removing zero covariates (no variability)
nsv <- nearZeroVar(training, saveMetrics = TRUE)
nsv # sex and religion are near zeros

# spline basis (for fitting curvy lines), in the splines package
library(splines)
bsBasis <- bs(training$age, df = 3)
bsBasis # give you columns for variable, variable squared, variable cubed so if you include them then you have a higher order model
lm1 <- lm(wage~bsBasis, data = training)
plot(training$age, training$wage, pch = 19, cex = .5)
point(training$age, predict(lm1, newdata = training), col = "red", pch = 19, cex = .5) #gives you the curved relationship
# remember that whatever you do you must also do on the test set
predict(bsBasis, age = testing$age)

```

### Principle Components Analysis
What do you do if you have multiple quantitative variables, many of which are highly correlated, yet you want to capture the information in these variables?

```{r}
# correlated predictors
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y = spam$type, 
                               p = .75, list = FALSE)
training <- spam[inTrain, ]
testing <- spam[-inTrain, ]

M <- abs(cor(training[, -58])) # leave out the 58th column, take corr and the absolute value
diag(M) <- 0 # every variable has a corr of 1 with itself so remove them
which(M > .8, arr.ind = T) # find the variables with a corr > .8
```

From the example above, you have two variables that are highly correlated. Including both in the model isn't very useful. We can use a weighted combination of predictors with PCA. The benefit of doing so is to reduce the number of predictors and we have reduced noise (due to averaging). This weighted combination will be a single variable that accounts for most of what is going on. One way to accomplish this is to take the sum of the two variables and  call that X and then take the difference and call it Y. Most of your Y values are 0, reducing a lot of the noise. When you have multiple variables, you'll find a new set of multivariate variables that are uncorrelated and explain as much variance as possible. If you put all of the variables together in one matrix, then you can find the best matrix created with fewer variables (lower rank) that explains the original data. The first goal is *statistical* the second is *data compression*.

There are two related solutions, both related to each other: SVD and PCA. In SVD, if X is a matrix with each variable in a column and each observation in a row, then the SVD is a matrix decomposition $$ X = UDV^t$$ where the columns of U are orthogonal (left singular vectors), then columns of V are orthogonal (right singular vectors) and D is a diagonal matrix (singular values). 

In PCA the principal components are equal to the right singular values if you first scale (subtract the mean, divide by the standard deviation) the variables. 
```{r}
# principle components in R
smallSpam <- spam[, c(34, 32)]
prComp <- prcomp(smallSpam) # perform just on the two variables that are correlated, should look like a sum/difference 
plot(prComp$x[, 1], prComp$x[, 2])
#prcomp() will work with many variables as well.

# rotation matrix will tell you how its summing up the variables to get the two components
prComp$rotation # shows you why he used .71 earlier

# with the spam data and more variables
typeColor <- ((spam$type == "spam")*1+1) # color is black if not spam, red if spam
prComp <- prcomp(log10(spam[, -58] + 1)) # principle comp for the entire dataset (the log and +1 is to make the data more gaussian)
plot(prComp$x[, 1], prComp$x[, 2], col = typeColor, xlab = "PC1", ylab = "PC2")
#PC1 and PC2 will explain the most and second most variability in the dataframe, respectively
```

```{r}
# PCA in caret
preProc <- preProcess(log10(spam[, -58] + 1), method = "pca", pcaComp = 2)
spamPC <- predict(preProc, log10(spam[, -58] + 1))
plot(spamPC[, 1], spamPC[, 2], col = typeColor)

# relate training variable to the principle components
trainPC <- predict(preProc, log10(training[, -58] + 1))
modelFit <- train(training$type~., method = "glm", data = trainPC)
# here the data is only the PCA

# when working with the test set, you must use the same preprocessing
testPC <- predict(preProc, log10(testing[, -58] + 1))
confusionMatrix(testing$type, predict(modelFit, testPC))

```

```{r}
# principle components and train the model in 1 step
modelFit <- train(training$type~., method = "glm", preProcess = "pca", data = training)
confusionMatrix(testing$type, predict(modelFit, testing))
```

PCA is best for linear type models. PCA can make it hard to interpret the predictors. Watch out for outliers, they can wreak havoc on the the outcome. 

### Predicting with Regression
One of the most direct ways to perform machine learning is with regression. Idea: fit a simple regression model, plug in new covariates and multiply them by the coefficients. This is most useful when the linear model is nearly correct. This approach is easy to implement and interpret, but performs poorly in non-linear settings.

```{r}
# example with old faithful eruptions
library(caret); data(faithful); set.seed(333)
inTrain <- createDataPartition(y = faithful$waiting, p = .5, list = FALSE)
trainFaith <- faithful[inTrain, ]
testFaith <- faithful[-inTrain, ]
head(trainFaith)
plot() # the relationship is basically linear
lm1 <- lm(eruptions$waiting, data = trainFaith)
summary(lm1)

plot(trainFaith$waiting, trainFaith$eruptions, col = "blue", xlab = "waiting")
lines(trainFaith$waiting, lm1$fitted, lwd = 3)
# not exact, but pretty good

# predict a new variable
coef(lm1)[1] + coef(lm1)[2]*80 # an estimate for waiting = 80
newdata <- data.frame(waiting = 80)
predict(lm1, newdata)
# you can pass the test data as the newdata and then plot to see how good the model works

# what if you want errors?
sqrt(sum((lm1$fitted - trainFaith$eruptions)^2))
# RMSE on the test set 
sqrt(sum((predict(lm1, newdata = testFaith) - trainFaith$eruptions)^2))

# prediction intervals
pred1 <- predict(lm1, newdata = testFaith, interval = "prediction")
ord <- order(testFaith$waiting)
plot(testFaith$waiting, testFaith$eruptions, pch = 19, col = "blue")
matlines(testFaith$waiting[ord], pred1[ord, ], type = "l", col = c(1, 2, 2), lty = c(1,1,1), lwd = 3)

# with caret
modFit <- train(eruptions~waiting, data = trainFaith, method = "lm")
summary(modFit$finalModel)

```

### Predicting with Regression: multiple covariates
We will try to predict the wages of people that come from various areas. 
```{r}
library(ISLR); library(ggplot2); library(caret)
data(Wage); Wage <- subset(Wage, select = -c(logwage))
summary(Wage)

#get test and training sets
inTrain <- createDataPartition(y = Wage$wage, p = .7, list = FALSE)
training <- Wage[inTrain, ]; testing <- Wage[-inTrain, ]
dim(training); dim(testing)

featurePlot(x = training[, c("age", "education", "jobclass")], 
            y = training$wage, plot = "pairs") # note a few grouping variables

#plot variables vs. wage
qplot(age, wage, data = training) # points at the top?
qplot(age, wage, color = jobclass, data = training) # jobclass shows that the higher points are from the information group
qplot(age, wage, color = education, data = training) # advanced degrees at the top

# fit a linear model
modFit <- train(wage~age + jobclass + education, method = "lm", data = training) # because jobclass and education are factor variables, train will make them indicator variables by default
finMod <- modFit$finalModel
print(modFit)

# look at diagnostics
plot(finMod, 1, pch = 19, cex = .5, col = "00000010")
# you want to see the red line centered at 0, notice that there are still a few outliers

# color by variables not used in the model (race)
qplot(finMod$fitted, finMod$residuals, color = race, data = training)

# plot fitted residuals vs. index (row number)
plot(finMod$residuals, pch = 19) # higher row numbers seem to be outliers (suggests there is a variable missing like time or age or something else that orders the data)

#wage vs. predicted
pred <- predict(modFit, testing)
qplot(wage, pred, color = year, data = testing)

# if you want to use all covariates in your model
modFitAll <- train(wage~., data = training, method = "lm") # the . means run it on everything
pred <- predict(modFitAll, testing)
qplot(wage, pred, data = testing)
```

## Week 3
### Predicting with Trees
If you have a bunch of variables to predict an outcome, you can split each variable into groups and then evaluate the homogeneity within each group. You split again if necessary, or if you decide that they are split enough then you stop. They are easy to interpret and perform better in non-linear settings. Unfortunately, without pruning/cross-validation, you can overfit. Its also harder to assess uncertainty and the results may be inconsistent.

Basic algorithm.  
    1. Start with all variables in one group
    2. Find the variable/split that best separates the outcomes (you want two different homogeneous leaves)
    3. Divide the data into two groups (leaves) on that split ("node")
    4. Within each split, find the best variable/split that separates the outcomes
    5. Continue until the groups are too small or sufficiently "pure"
    
Measures of impurity
See chart (m is leaf, k is the class)

Deviance (base e)/Information Gain (base 2)
  Sum of the probabilities times log(probability)
  0 = perfect purity
  1 = no purity
  
Example with iris
```{r}
data(iris); library(ggplot2)
names(iris)
# we are trying to predict the species
table(iris$Species) # 50/50/50
inTrain <- createDataPartition(y = iris$Species, p = .7, list = FALSE)
training <- iris[inTrain, ]
testing <- iris[-inTrain, ]
dim(training); dim(testing)
qplot(Petal.Width, Sepal.Width), colour = Species, data = training)
# there are 3 distinct species here
modFit <- train(Species ~., method = "rpart", data = training)
print(modFit$finalModel)
# you get all of the nodes and how they are split, along with the prob for each split
# example, if the petal.length is less than 2.45, then it belongs to setosa 100%
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree") #dendogram
text(modFit$finalModel, use.n = TRUE, all = TRUE, cex = .8)
library(rattle)

# predict new values
predict(modFit, newdata = testing)

fancyRpartPlot(modFit$finalModel)
```

Classification trees are nonlinear models. They use interactions between variables. Data transformations may be less important. Trees can also be used for regression problems (continuous outcome).

### Bagging. 
Bootstrap aggregating. The idea is when you fit complicated models, if you average them together you get a smoother fit that provides a better estimate of bias/variance. Basic idea:
    1. Resample cases (with replacement) and recalculate predictions
    2. Average or majority vote
    Notes: similar bias, reduced variance, more useful for non-linear functions
    
```{r}
library(ElemStatLearn); data(ozone, package = "ElemStatLearn")
ozone <- ozone[order(ozone$ozone), ]
head(ozone)

# bagged loess, a smooth curve you can fit into the data
# resample data 10 times, fit a curve and average values
ll <- matrix(NA, nrow = 10, ncol = 155)
for(i in 1:10) {
    ss <- sample(1:dim(ozone)[1], replace = T)
    ozone0 <- ozone[ss, ]; ozone0 <- ozone0[order(ozone$ozone), ]
    loess0 <- loess(temperature ~ ozone, data = ozone0, span = .2)
    ll[i, ] <- predict(loess0, newdata = data.frame(ozone = 1:155))
}
plot(ozone$ozone, ozone$temperature, pch = 19, cex = .5)
for(i in 1:10){lines(1:155, ll[i, ], col = "grey", lwd = 2)}
lines(1:155, apply(ll, 2, mean), col = "red", lwd = 2)

# bagging in caret (use method options under train())
  # bagEarth, treebag, bagFDA
  # or...bag()

# alternatively you can build your own bag (see example)

```
Bagging is most useful for non-linear models. Its also often used with trees and an extension is random forests. Several models in the caret `train()` function use bagging.

### Random Forest.
Think of these as an extension of bagging. The process is as follows:
  1. Bootstrap samples, create decision trees
  2. At each split, bootstrap variables (only a subset is considered)
  3. Grow multiple trees and vote
  
Random forests are very accurate, but they take a long time, can increase overfitting, and interpreting them is hard. New predictions are run through each tree and you average the results.

```{r}
data(iris); library(ggplot2); library(caret)
inTrain <- createDataPartition(y = iris$Species, p = .7, list = FALSE)
training <- iris[inTrain, ]
testing <- iris[-inTrain, ]
modFit <- train(Species~., data = training, method = "rf", prox = TRUE) # prox gives you a little more information
modFit

# getting a single tree
getTree(modFit$finalModel, k = 2) # k = 2 gives you the second tree
# each row is a particular split, the variable you are splitting on, the value, and the prediction

# class "centers"
# you can use centers information to find the centers of the class predictions
irisP <- classCenter(training[, c(3, 4)], training$Species, modFit$finalModel$prox)
irisP <- as.data.frame(irisP); iris$Species <- rownames(irisP)
p <- qplot(Petal.Width, Petal.Length, col = Species, data = training)
p + geom_point(aes(x = Petal.Width, y = Petal.Length, col = Species), size = 5, shape = 4, data = irisP)

# Predicting new values
pred <- predict(modFit, testing); testing$predRight <- pred == testing$Species
table(pred, testing$Species)
qplot(Petal.Width, Petal.Length, color = predRight, data = testing, main = "newdata Predictions") # shows where you were off

```

Random forests and boosting are usually two of the most popular algorithms in prediction contests. Interpretability is difficult.

### Boosting
Take a large number of possibly weak predictors, weight their strengths and add them up to get a stronger predictor. This is very similar to bagging and random forest.

Start with k classifieers, for example all possible trees or all possible regression models and all possible cutoffs. Next, create a classifier that combines the classification functions. The goal is to minimize the error on the training set. See adaboost on wikipedia or the webee.tehnion.ac.il tutorial.

Boosting can be done with any set of week classifiers. One large subclass of boosting is gradient boosting. R has multiple boosting libraries, but most are available in caret with `train()`.
```{r}
library(ISLR); data(Wage); library(ggplot2); library(caret)
Wage <- subset(Wage, select = -c(logwage)) # removes logwage
inTrain <- createDataPartition(Wage$wage, p = .7, list = FALSE)
training <- Wage[inTrain, ]; testing <- Wage[-inTrain, ]

# fit the model
modFit <- train(wage~., method = "gbm", data = training, verbose = FALSE)
print(modFit) # the trees are used together to produce the boosted result
qplot(predict(modFit, testing), wage, data = testing)
```

### Model Based Prediction
Now, we assume the data follow a probabilistic model. We want to use Bayes's Theorem to identify optimal classifiers. This way we can take advantage of the structure of the data and the approach may be computationally convenient and its reasonably accurate on real problems. Cons: we are making assumptions about the data and when the model is incorrect, you may get reduced accuracy.

Our goal is to build a parametric model for the P(y = k|X = x), where k is the specific class and X is our set of predictors. We can apply bayes theorem... see chart

The priors (pik) are set by data in advance. Usually your fk(x) is a Guassian distribution. Estimate the mean and the sd from the data. Classify to the class with the highest value of Probability. Linear disciminant analysis is the usual model for this approach, but there are others. Naive bayes assumes independence between features for model building.

```{r}
# Iris data example
data(iris); library(ggplot2)
names(iris)iris
table(iris$Species)
inTrain <- createDataPartition(y = iris$Species, p = .7, list = FALSE)
training <- iris[inTrain, ]
testing <- iris[-inTrain, ]
dim(training); dim(testing)

# build predictions
modlda <- train(Species~., data= training, method = "lda")
modnb <- train(Species~., data = training, method = "nb")
plda <- predict(modlda, testing); pnb <- predict(modnb, testing)
table(plda, pnb)

# compare results
equalPredictions <- (plda == pnb)
qplot(Petal.Width, Sepal.Width, color = equalPredictions, data = testing)

```

```{r}
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
```

## Week 4
### Regularized Regression
Basic idea here is to fit a regression model, then penalize or shrink the large variables. The rationale here is that we will help with the bias/variance (larger coeff's have larger variance) trade off and it can help with model selection. On large datasets RR is computationally demanding and does not perform as well as random forests and boosting. If X1 and X2 are nearly perfectly correlated, we can approximate a model with only X1 by multiplying it times the sum of X1 and X2. You will get a good estimate of Y, but it will be a little biased and you will get significantly less variance (due to the correlation).

```{r}
library(ElemStatLearn); data(prostate)
str(prostate)

# you want to predict psa, when you build a model with all possible combinations of predictors, the error will decrease (on the training set). The test set error goes down, but then goes back up due to overfitting. The test set error plateaus and then increases.
```

When you have data and computational resources, split the samples. Approach:  
  1. Divide training/test/validation
  2. Treat validation as test data, training all competing models on the train data an dpick the best one on validation
  3. To appropriately assess performance on new data apply to test set
  4. You may re-split and reperform steps 1-3

Another approach is to decompose the prediction error to only include important variables == irreducible error + bias^2 + variance
We want to reduce this last overall quantity.

```{r}
small <- prostate[1:5, ]
lm(lpsa~., data = small)
# you get a bunch of NAs because you have more predictors than samples.
```

Hard Threshholding: see slide 10...

Regularized Regression. If the Bj's are unconstrained (no form), they can grow and garner high variance, but to control, you can regularize/shrink the coefficients with PRSS(B) = SUM(Yj - SUMB1*Xij)^2 + P(lambda, B) (minimizes the distance between the model and the prediction squared, and then if the coefficients are large shrink them back down). where PRSS is a penalized form of the sum of squares. Things that are commonly looked for include Penalty reduces complexity, penalty reduces variance, penalty respects structure of the problem. 

Ridge Regression...

You get a coefficient path where you compare the coefficients to lambda values. You see a conversion to 0 as lambda grows. Lambda is your tuning parameter and determines the amount of regularization. A lambda of 0 is the same as the linear model. To pick the paramater, use CV or other techniques. 

Lasso is very similar. 

See Hector Corrada Bravos PML lecture notes. The Elements of Statistical Learning also has good notes. In Caret, use methods ridge, lasso and relaxo to accomplish. 

### Combining Predictors - Ensembling Methods
You can combine classifiers by averaging/voting. Combining classifiers improves accuracy and reduces interpretability. Boosting, bagging, and random forests are all variants on this theme. Basically, we are averaging classifiers together.

The basic intuition is a majority vote. Say you have 5 independent classifiers. If the accuracy is 70% for each, then the majority vote would be 83.7%. With 101 classifiers, the accuracy is 99.9%. If you use bagging, boosting, random forests, then you are combining similar classifiers. Model stacking and model ensembling is combining different classifiers. We will do model stacking in this lecture. 

```{r}
library(ISLR); data(Wage); library(ggplot2); library(caret)
Wage <- subset(Wage, select = -c(logwage))

# create a building data set and validation set
inBuild <- createDataPartition(y = Wage$wage, p = .7, list = FALSE)
validation <- Wage[-inBuild, ]; buildData <- Wage[inBuild, ]
inTrain <- createDataPartition(y = buildData$wage, p = .7, list = FALSE)
training <- buildData[inTrain, ]; testing <- buildData[-inTrain, ]
# we are trying to predict wage, so log wage is a very good predictor

dim(training)
dim(testing)
dim(validation)

# two separate models
mod1 <- train(wage~., method = "glm", data = training)
mod2 <- train(wage~., method = "rf", data = training, trControl = trainControl(method = "cv"), number = 3)

# look at the predictions next to each other
pred1 <- predict(mod1, testing); pred2 <- predict(mod2, testing)
qplot(pred1, pred2, colour = wage, data = testing)

# fit a model that combines the predictors
predDF <- data.frame(pred1, pred2, wage = testing$wage)
combModFit <- train(wage~., method = "gam", data = predDF)
combPred <- predict(combModFit, predDF)

# testing errors
sqrt(sum((pred1-testing$wage)^2))
sqrt(sum((pred2-testing$wage)^2))
sqrt(sum((combPred-testing$wage)^2)) # this is best

# predict on validation set
pred1V <- predict(mod1, validation); pred2V <- predict(mod2, validation)
predVDF <- data.frame(pred1 = pred1V, pred2 = pred2V)
combPredV <- predict(combModFit, predVDF)

sqrt(sum((pred1V - validation$wage)^2))
sqrt(sum((pred2V - validation$wage)^2))
sqrt(sum((combPredV - validation$wage)^2)) # performs best
```

Even simple blending can be useful. Scalability matters. These models can be very computationally intensive, so they often arent the best models to use.

### Forecasting.
Typically applied to time series data, stock movement, etc. Forecasting introduces some specific types of dependence structure: data is dependent over time. There are also some specific pattern types, such as trends (long term increase or decrease), seasonal patterns (patterns related to time of week, month, year, etc), and cycles (patterns that rise and fall periodicallly). Subsampling into the training/test sets is more complicated. Similar issues arise with spatial data (dependency to nearby observations, location specific effects). The typical goal is to predict one or more observations into the future. All standard predictions can be used (with caution). 

Beware of spurious correlations. Time series can be correlated for random reasons. In the future these things can be unrelated. Spatial data also frequently looks similar because population doesnt change. Extrapolation is also hard. Be ware of looking too far out into the future. 

```{r}
# forecasting with google data
library(quantmod)
from.dat <- as.Date("01/01/08", format = "%m/%d/%y")
to.dat <- as.Date("12/31/13", format = "%m/%d/%y")
getSymbols("GOOG", src = "google", from = from.dat, to = to.dat)
head(GOOG)
# provides open, high, low, close and volume information

mGoog <- to.monthly(GOOG)
googOpen <- Op(mGoog)
ts1 <- ts(googOpen, frequency = 12)
plot(ts1, xlab = "Years+1", ylab = "GOOG") # 7 year plot

```

As an example of time series decomposition, we can decompose the data into:  
  1. Trend: consistently increasing pattern over time
  2. Seasonal: when there is a pattern over a fixed time period of time that recurs
  3. Cyclic: when data rises and falls over non-fixed periods
  
To decompose, use the `decompose()` function.
```{r}
plot(decompose(ts1), xlab = "Years+1")
```

For training and test sets.
```{r}
ts1Train <- window(ts1, start = 1, end = 5)
ts1Test <- window(ts1, start = 5, end = (7-.01))
ts1Train

# simple moving average - average the dv up to that time
plot(ts1Train)
lines(ma(ts1Train, order = 3), col = "red")

# exponential smoothing
# yhat(t+1) = alpha(yt) + (1-alpha)yhat(t-1)
# you are weighting nearby timepoints 
ets1 <- ets(ts1Train, model = "MMMM")
fcast <- forecast(ets1)
plot(fcast); lines(ts1Test, col = "red")

# accuracy
accuracy(fcast, ts1Test)

```

For additional information, see Rob Hyndman's "Forecasting: Principles and Practice". Highly recommended and free online. Also, see the quantmod and quandl packages for finance related problems.

### Unsupervised Prediction
Up until this point, we know the labels. They are all supervised. We need to discover the labels. We will now create clusters, name them, and build predictors for them. In the new dataset, you could now predict the clusters. There is still noise, and interpreting the clusters is often difficult.

```{r}
# example with iris
data(iris); library(ggplot2)
inTrain <- createDataPartition(y = iris$Species, p= .7, list = FALSE)
training <- iris[inTrain, ]
testing <- iris[-inTrain, ]
dim(training); dim(testing)

#cluster with kmeans
kMeans1 <- kmeans(subset(training, select = -c(Species)), centers = 3) #ignores species
training$clusters <- as.factor(kMeans1$cluster)
qplot(Petal.Width, Petal.Length, color = clusters, data = training)

table(kMeans1$cluster, training$Species)
modFit <- train(clusters~., data = subset(training, select = -c(Species)), method = "rpart")
table(predict(modFit, training), training$Species)

# apply on test set
testClusterPred <- predict(modFit, testing)
table(testClusterPred, testing$Species) # it does pretty good
```

Beware of overinterpretation. See ESL for more information (of course). This is one basic approach to "recommendation engines".
